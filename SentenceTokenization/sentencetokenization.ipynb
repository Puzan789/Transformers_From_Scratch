{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: d:\\Transformers_From_scratch\n",
      "Python path: ['C:\\\\Users\\\\puzan12\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip', 'C:\\\\Users\\\\puzan12\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs', 'C:\\\\Users\\\\puzan12\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib', 'C:\\\\Users\\\\puzan12\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312', 'd:\\\\Transformers_From_scratch\\\\venv', '', 'd:\\\\Transformers_From_scratch\\\\venv\\\\Lib\\\\site-packages', 'd:\\\\Transformers_From_scratch\\\\venv\\\\Lib\\\\site-packages\\\\win32', 'd:\\\\Transformers_From_scratch\\\\venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'd:\\\\Transformers_From_scratch\\\\venv\\\\Lib\\\\site-packages\\\\Pythonwin', 'd:/Transformers_From_scratch']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the working directory to the project root\n",
    "project_root = 'd:/Transformers_From_scratch'\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Verify the current working directory and Python path\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Python path:\", sys.path)\n",
    "\n",
    "# Now import the Transformer class\n",
    "from BuildingaTransformer.transformer import Transformer\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.nn as nn \n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_file=\"dataset/english.txt\"\n",
    "kannada_file=\"dataset/kannada.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<START>'\n",
    "PADDING_TOKEN = '<PADDING>'\n",
    "END_TOKEN = '<END>'\n",
    "\n",
    "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ', \n",
    "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', \n",
    "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', \n",
    "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', \n",
    "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', \n",
    "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', \n",
    "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', \n",
    "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', \n",
    "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', \n",
    "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', \n",
    "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
    "\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@', \n",
    "                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', \n",
    "                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', \n",
    "                        'Y', 'Z',\n",
    "                        '[', '\\\\', ']', '^', '_', '`', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ಆ', 'ಘ', 'ಾ', 'ತ', 'ಕ', 'ಾ', 'ರ', 'ಿ', 'ಯ', 'ಾ', 'ದ', 'ು', 'ದ', 'ು']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=list(\"ಆಘಾತಕಾರಿಯಾದುದು\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_kannada={k:v for k,v in enumerate(kannada_vocabulary)}\n",
    "kannada_to_index={v:k for k,v in enumerate(kannada_vocabulary)}\n",
    "index_to_english={k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index={v:k for k,v in enumerate(english_vocabulary)}\n",
    "# its just mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (english_file, 'rb') as f:\n",
    "    english_sentences=f.readlines()\n",
    "with open(kannada_file,'rb') as f:\n",
    "    kannada_sentences=f.readlines()\n",
    "\n",
    "# num of sentences\n",
    "TOTAL_SENTENCES=10000\n",
    "english_sentences=english_sentences[:TOTAL_SENTENCES]\n",
    "kannada_sentences=kannada_sentences[:TOTAL_SENTENCES]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = [sentences.decode('utf-8').rstrip('\\n') if isinstance(sentences, bytes) else sentences.rstrip('\\n') for sentences in english_sentences]\n",
    "kannada_sentences = [sentences.decode('utf-8').rstrip('\\n') if isinstance(sentences, bytes) else sentences.rstrip('\\n') for sentences in kannada_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hes a scientist.',\n",
       " \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
       " '8 lakh crore have been looted.',\n",
       " 'I read a lot into this as well.',\n",
       " \"She was found dead with the phone's battery exploded close to her head the following morning.\"]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468\n",
      "\n",
      "Union Minister of Agriculture & Farmers Welfare, Rural Development &Panchayati Raj and Food Processing Industries, Shri Narendra Singh Tomar today launched AYUSHMAN SAHAKAR, a unique scheme to assist cooperatives play an important role in creation of healthcare infrastructure in the country formulated by the apex autonomous development finance institution under the Ministry of Agriculture and Farmers Welfare, the National Cooperative Development Corporation (NCDC)\n",
      "491\n"
     ]
    }
   ],
   "source": [
    "print(max(len(x) for x in english_sentences))\n",
    "print()\n",
    "max_sentence = max(english_sentences, key=len)\n",
    "print(max_sentence)\n",
    "print(max(len(x) for x in kannada_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95th percentile of english : 151.0\n",
      "95th percentile of kannada  : 149.0\n"
     ]
    }
   ],
   "source": [
    "percentile=95\n",
    "print(f\"{percentile}th percentile of english : {np.percentile([len(x) for x in english_sentences],percentile)}\")\n",
    "print(f\"{percentile}th percentile of kannada  : {np.percentile([len(x) for x in kannada_sentences],percentile)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it means 95 percent have less than 151 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  10000\n",
      "Number of sentences:  10000\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 200\n",
    "\n",
    "def is_valid_token(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) <= (max_sequence_length - 1)\n",
    "\n",
    "valid_sentences_indexes = []\n",
    "for index in range(len(kannada_sentences)):\n",
    "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
    "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
    "    and is_valid_length(english_sentence, max_sequence_length) \\\n",
    "    and is_valid_token(kannada_sentence, kannada_vocabulary):\n",
    "        valid_sentences_indexes.append(index)\n",
    "\n",
    "print(\"Number of sentences: \", len(kannada_sentences))\n",
    "print(\"Number of sentences: \", len(english_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid_sentences:  8159\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of valid_sentences: \", len(valid_sentences_indexes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences=[english_sentences[i] for i in valid_sentences_indexes]\n",
    "kannada_sentences=[kannada_sentences[i] for i in valid_sentences_indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model=512\n",
    "batch_size=30\n",
    "ffn_hidden=2048\n",
    "num_heads=8\n",
    "drop_prob=0.1\n",
    "num_layers=1\n",
    "max_sequence_length=200\n",
    "kn_vocab_size=len(kannada_vocabulary)\n",
    "transformer=Transformer(d_model,\n",
    "                        ffn_hidden,\n",
    "                        num_heads,\n",
    "                        drop_prob,\n",
    "                        num_layers,\n",
    "                        max_sequence_length,\n",
    "                        kn_vocab_size,\n",
    "                        english_to_index,\n",
    "                        kannada_to_index,\n",
    "                        START_TOKEN,\n",
    "                        END_TOKEN,\n",
    "                        PADDING_TOKEN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(97, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialEncoder(\n",
       "      (0): EncoderLayer(\n",
       "        (attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (sentence_embedding): SentenceEmbedding(\n",
       "      (embedding): Embedding(125, 512)\n",
       "      (position_encoder): PositionalEncoding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): SequentialDecoder(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): MultiHeadAttention(\n",
       "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNormalization()\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
       "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
       "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNormalization()\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (ffn): PositionwiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm3): LayerNormalization()\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=125, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "class TextDatasets(Dataset):\n",
    "    def __init__(self,english_sentences,kannada_sentences):\n",
    "        self.english_sentences = english_sentences\n",
    "        self.kannada_sentences = kannada_sentences\n",
    "    def __len__(self):\n",
    "        return len(self.english_sentences)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.english_sentences[index],self.kannada_sentences[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=TextDatasets(english_sentences,kannada_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8159"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
       " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=3\n",
    "train_loader=DataLoader(dataset,batch_size)\n",
    "iterator=iter(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "English: ('Hes a scientist.', \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\", '8 lakh crore have been looted.')\n",
      "Kannada: ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.')\n"
     ]
    }
   ],
   "source": [
    "for batch_num,batch in enumerate(iterator):\n",
    "    english_batch,kannada_batch=batch\n",
    "    print(f'Batch {batch_num}')\n",
    "    print('English:', english_batch)\n",
    "    print('Kannada:', kannada_batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "def tokenization(sentence,language_to_index,start_token=True,end_token=True):\n",
    "    sentences_word_indexes=[language_to_index[token]for token in list(sentence)]\n",
    "    if start_token:\n",
    "        sentences_word_indexes.insert(0,language_to_index[START_TOKEN])\n",
    "    if end_token:\n",
    "        sentences_word_indexes.append(language_to_index[END_TOKEN])\n",
    "    for _ in range(len(sentences_word_indexes),max_sequence_length):\n",
    "        sentences_word_indexes.append(language_to_index[PADDING_TOKEN])\n",
    "    return torch.tensor(sentences_word_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],\n",
    "                                reduction='none')\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hes a scientist.',\n",
       "  \"'But we speak the truth aur ye sach hai ke Gujarat mein vikas pagal hogaya hai,'' Rahul Gandhi further said in Banaskantha\",\n",
       "  '8 lakh crore have been looted.'),\n",
       " ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
       "  '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
       "  'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenized,kn_tokenized=[],[]\n",
    "for sentence_num in range(batch_size) :\n",
    "    eng_sentences,kn_sentence=batch[0][sentence_num],batch[1][sentence_num]\n",
    "    eng_tokenized.append(tokenization(eng_sentences, english_to_index,start_token=False,end_token=False))\n",
    "    kn_tokenized.append(tokenization(kn_sentence, kannada_to_index, start_token=True, end_token=True))\n",
    "eng_tokenized=torch.stack(eng_tokenized)\n",
    "kn_tokenized=torch.stack(kn_tokenized)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40, 69, 83,  1, 65,  1, 83, 67, 73, 69, 78, 84, 73, 83, 84, 15, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95],\n",
       "        [ 8, 34, 85, 84,  1, 87, 69,  1, 83, 80, 69, 65, 75,  1, 84, 72, 69,  1,\n",
       "         84, 82, 85, 84, 72,  1, 65, 85, 82,  1, 89, 69,  1, 83, 65, 67, 72,  1,\n",
       "         72, 65, 73,  1, 75, 69,  1, 39, 85, 74, 65, 82, 65, 84,  1, 77, 69, 73,\n",
       "         78,  1, 86, 73, 75, 65, 83,  1, 80, 65, 71, 65, 76,  1, 72, 79, 71, 65,\n",
       "         89, 65,  1, 72, 65, 73, 13,  8,  8,  1, 50, 65, 72, 85, 76,  1, 39, 65,\n",
       "         78, 68, 72, 73,  1, 70, 85, 82, 84, 72, 69, 82,  1, 83, 65, 73, 68,  1,\n",
       "         73, 78,  1, 34, 65, 78, 65, 83, 75, 65, 78, 84, 72, 65, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95],\n",
       "        [25,  1, 76, 65, 75, 72,  1, 67, 82, 79, 82, 69,  1, 72, 65, 86, 69,  1,\n",
       "         66, 69, 69, 78,  1, 76, 79, 79, 84, 69, 68, 15, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95,\n",
       "         95, 95]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenized# 95 is paddinf tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  43,  86,  82,  96,   1,  89, 111,  87, 104,  74,  56,   1,  89,\n",
       "         106,  86,  79,  93,  86,  73,  86,  82,  96,  15, 124, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123],\n",
       "        [  0,   3,  42,  73,  82, 100,   1,  89,  71, 106,  81,   1,  90, 103,\n",
       "          82,   1,  78, 111,  73, 101,   1,  78,  82,  96,  71, 106,  71,  73,\n",
       "         100,   1,  50, 111,  73,  96,   1,  90, 101,  85,  94,  73,   1,  82,\n",
       "          93,  90,  96,  84, 106,   1,  58,  93, 111,  74,  94,  13,   1,   3,\n",
       "           3,  89,  97,  82,  71, 106,   1,  63,  75,  82,  96,   1,  61,  95,\n",
       "          75,  93,  73,   1,  63,  71, 100,   1,  89, 106,  76,  82, 106,  74,\n",
       "         100,   1,  75,  68, 100,  89,  96,  71, 106,  71,  94,  73, 106,  73,\n",
       "          93,  82, 100,   3, 124, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123],\n",
       "        [  0,  56,  85, 106,  85,  71,  75,  86,  93,  58,  94,  73, 106,  73,\n",
       "           1,  25,   1,  84,  56, 106,  88,   1,  82,  97,  15, 124, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123, 123,\n",
       "         123, 123, 123, 123]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_tokenized# 0 is start token #124 is end token  and 123 is padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, kn_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 3.0909762382507324\n",
      "English: Hes a scientist.\n",
      "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
      "Kannada Prediction: ದದಾ್ ದೆದಿದಿುದುದಿಿದಿುಿು \n",
      "Evaluation translation (should we go to the mall?) : ('ಅದ್ದೆ ಅದ್ರ್ದೆ ಬೆ ಹಿದು ಹು ಬು ಹಿದು.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 100 : 2.8821327686309814\n",
      "English: I am not saying this.\n",
      "Kannada Translation: ಈ ಮಾತನ್ನು ನಾನಲ್ಲ.\n",
      "Kannada Prediction: ನತಕ್ತಿುತ್ ಮ್ತು್ರ್\n",
      "Evaluation translation (should we go to the mall?) : ('ಇದು ಮಿತ್ತ್ತ್ತ್ರು ಕ್ತ್ತ್ತ್ತ್ತ್ತಿದು.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 200 : 2.8839213848114014\n",
      "English: Do you know of fellow believers who are beset by trials?\n",
      "Kannada Translation: ಕೆಲವರು ಹಿಂಸೆಯನ್ನು ಎದುರಿಸುತ್ತಿರಬಹುದು.\n",
      "Kannada Prediction: ಅಾ ್ೆ್ ಬಾ ಗಿ  ್ರಿ ಪಲ  ್ ಾ ್ಲ್ ೆಿ್ ್ \n",
      "Evaluation translation (should we go to the mall?) : ('ಅವಿಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 300 : 2.817718982696533\n",
      "English: It is accomplished by preaching and teaching the Word of God.\n",
      "Kannada Translation: ಈ ಹರ್ಷಭರಿತ ಕೆಲಸವು ನಮಗೆ ಮತ್ತು ನಾವು ಯಾರಿಗೆ ಬೋಧಿಸುತ್ತೇವೋ ಅವರಿಗೆ ಪ್ರಯೋಜನ ತರುವುದು.\n",
      "Kannada Prediction: ನ ನಿ್ನ್ಿಿ ್ಮ್ ್್್ ಪ್ಾಿ ಪನಿಲ್ ಕಿನಾ ಮಂಗಿ ಿ ಮಿನಿ ಿ ್ನಿರಾಲಸರಿಿದಾ.ಪ್ರಿಿದಿ್ನಿಿ ಿ ಿ \n",
      "Evaluation translation (should we go to the mall?) : ('ಅದರು ಕಿದಿ ಕ್ಲಿ ಕಿದಿದಿದಿದಿದಿದಿದಿದಿದಿದಿದಿ.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 400 : 3.0067992210388184\n",
      "English: Chop the chilli as fine as possible.\n",
      "Kannada Translation: ಬೆಳ್ಳುಳ್ಳಿ ಸಾಧ್ಯವಾದಷ್ಟು ನುಣ್ಣಗೆ ಸಾಧ್ಯವಾದಷ್ಟು ಕತ್ತರಿಸು.\n",
      "Kannada Prediction: ಅಾರುರು ು   ಕಿರುರಾು ುುರೆ ಮು ುರ ು ಸುರಾರಾುರು್ಲ್ ಮ್ುತುುದು \n",
      "Evaluation translation (should we go to the mall?) : ('ಅದು ಮಾಗು ಮಾಗು ಮಾಗು ಕು ಮಾಗು ಕು ಮಾಗು.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 500 : 2.7513670921325684\n",
      "English: The switch to copper plates was probably made in Italy, and thereafter etching soon came to challenge engraving as the most popular medium for artists in printmaking.\n",
      "Kannada Translation: ತಾಮ್ರದ ಹಲಗೆಗಳಿಗೆ ಬದಲಾವಣೆಯನ್ನು ಬಹುಶಃ ಇಟಲಿಯಲ್ಲಿ ಮಾಡಲಾಗಿತ್ತು, ಮತ್ತು ಆ ನಂತರ ನಕಾಸೆ ಕೆತ್ತನೆಗೆ ಪ್ರತಿಸ್ಪರ್ಧಿಯಾಗಿ ಮುದ್ರಣ ತಯಾರಿಕೆಯಲ್ಲಿ ಎಚ್ಚಣೆಯು ಹೆಚ್ಚು ಜನಪ್ರಿಯ ಪ್ರಕಾರವಾಗಿ ಬಂದಿತು.\n",
      "Kannada Prediction: ಆ್ರ್ತ್ ತತ್್ ಿ್   ವ್ು್ರ್   ್ನ್ ಮ್್ ್ ಹತ್್  ್ತ್ ನಾನು್ ಾ ್ತ್  ಅ್್ ್ ನರವ್ತ್್ನ್ುರ್ ನ್ ್ತ್್ ್ ಅ್ ್್ದ್ತ್್ತ್ ್ನ್ಗಮ್ ್ ್್ಅ್್ಾ್ಯ್ ್್ತ್ ನರ್ಲುಿ   ಅ್ ್ರ್ ವ್್್ತ್ದುಹ್ಲು್ರ್್ರೆ ಸೆದ್ ್ \n",
      "Evaluation translation (should we go to the mall?) : ('ಇದ್ತ್ತ್ನ್ನ್ತ್ತ್ಲ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತು.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 600 : 2.7239158153533936\n",
      "English: What are you good at?\n",
      "Kannada Translation: ಏನು ಮಾಡುವುದರಲ್ಲಿ ನೀವು ಒಳ್ಳೆಯದು?\n",
      "Kannada Prediction: ಆ\"್ ಅಾರೆ ಿ  ಿ್ ್ ಹ್ದಾ ಮವುತಿ.ೆಿ.\n",
      "Evaluation translation (should we go to the mall?) : ('ಅದರ್ನ್ನ್ನ್ನ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 700 : 3.1628711223602295\n",
      "English: The Chief General Manager (HRD)\n",
      "Kannada Translation: ಡೆಪ್ಯುಟಿ ಜನರಲ್ ಮ್ಯಾನೇಜರ್ (ಎಚ್ಆರ್),\n",
      "Kannada Prediction: ಇಿ ್ಯ್ ಿ ಸ್್ಿ್ಯಸಿಯಿರು ೆಿತಮರಂ್ತಸುದ .                                                                                                                                                                     \n",
      "Evaluation translation (should we go to the mall?) : ('ಅದು ಸ್ಯಾಗೆ ಮಾರು ಮಾಗೆ ಮಾರು.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 800 : 2.752436637878418\n",
      "English: He was in India last week.\n",
      "Kannada Translation: ಕಳೆದ ವಾರ ನಾನು ಭಾರತಕ್ಕೆ ಹೋಗಿದ್ದೆ.\n",
      "Kannada Prediction: ಅ್್ ುಅಾರ್ಮ್ರ್ ಮಾರ್್್ಯ್.ಹಾಂುದುಲು\n",
      "Evaluation translation (should we go to the mall?) : ('ಅವರು ಮಾರ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ.<END>',)\n",
      "-------------------------------------------\n",
      "Iteration 900 : 3.1000335216522217\n",
      "English: The price of petrol was hiked by 13 paise/litre and the price of diesel was slashed by 12 paise/litre when the fuel suppliers last revised the prices on December 1, 2016\n",
      "Kannada Translation: ಇದಕ್ಕೂ ಮೊದಲು ಡಿಸೆಂಬರ್ 01ರಂದು ಪೆಟ್ರೋಲ್ ಬೆಲೆ 13 ಪೈಸೆ ಹಾಗೂ ಡೀಸೆಲ್ ಬೆಲೆ ಪ್ರತಿ ಲೀಟರ್ 12 ಪೈಸೆ ಏರಿಕೆಗೊಳಿಸಲಾಗಿತ್ತು\n",
      "Kannada Prediction: ನದ್್ರ್ ಮಾರರ್್ ಮಿರ್ ದಿ್ತಮಂನ್ದ್ ನ್ ್ಲಿನ್ ಮಿ ್ ಮಂ ಮ್ನ್ ನಾರಿ ನಿತಿ ್ ಮಿ ಿ ಮಿತು್ ಪ್ತ್ಿನಮನಿನಿತಿ ಮನಿತ್ ೆ ರ್ಸಿ್ಗಿತ್ತಿತ   ಿ                                                ನ  ವ              ಿ  ಿ                 \n",
      "Evaluation translation (should we go to the mall?) : ('ನಿನ್ನ್ ನ್ನ್ನ್ನ್ನ್ನ್ನ್ಲ್ಲ್ಲಿ ನಿದೆ.<END>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [200] at entry 0 and [201] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, kn_batch)\n\u001b[0;32m     13\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m kn_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mkn_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(kn_batch, start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(\n\u001b[0;32m     25\u001b[0m     kn_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kn_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     26\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\BuildingaTransformer\\transformer.py:302\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start_token, enc_end_token, dec_start_token, dec_end_token)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m    292\u001b[0m             x, \n\u001b[0;32m    293\u001b[0m             y, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    299\u001b[0m             dec_start_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[0;32m    300\u001b[0m             dec_end_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[0;32m    301\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, encoder_self_attention_mask, start_token\u001b[38;5;241m=\u001b[39menc_start_token, end_token\u001b[38;5;241m=\u001b[39menc_end_token)\n\u001b[1;32m--> 302\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_start_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_end_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\BuildingaTransformer\\transformer.py:265\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n\u001b[1;32m--> 265\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x, y, self_attention_mask, cross_attention_mask)\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\BuildingaTransformer\\transformer.py:70\u001b[0m, in \u001b[0;36mSentenceEmbedding.forward\u001b[1;34m(self, x, start_token, end_token)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, start_token, end_token): \u001b[38;5;66;03m# sentence\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     72\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoder()\u001b[38;5;241m.\u001b[39mto(get_device())\n",
      "File \u001b[1;32md:\\Transformers_From_scratch\\BuildingaTransformer\\transformer.py:66\u001b[0m, in \u001b[0;36mSentenceEmbedding.batch_tokenize\u001b[1;34m(self, batch, start_token, end_token)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batch)):\n\u001b[0;32m     65\u001b[0m    tokenized\u001b[38;5;241m.\u001b[39mappend( tokenize(batch[sentence_num], start_token, end_token) )\n\u001b[1;32m---> 66\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39mto(get_device())\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [200] at entry 0 and [201] at entry 2"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, kn_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
    "        optim.zero_grad()\n",
    "        kn_predictions = transformer(eng_batch,\n",
    "                                     kn_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start_token=False,\n",
    "                                     enc_end_token=False,\n",
    "                                     dec_start_token=True,\n",
    "                                     dec_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
    "        loss = criterian(\n",
    "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
    "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in kn_sentence_predicted:\n",
    "              if idx == kannada_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += index_to_kannada[idx.item()]\n",
    "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            kn_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_sequence_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          kn_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          enc_start_token=False,\n",
    "                                          enc_end_token=False,\n",
    "                                          dec_start_token=True,\n",
    "                                          dec_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_kannada[next_token_index]\n",
    "                kn_sentence = (kn_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # everything is summed\n",
    "# class SentenceEmbedding(nn.Module):\n",
    "#     \"For a given sentence, create an embedding\"\n",
    "#     def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
    "#         super().__init__()\n",
    "#         self.vocab_size = len(language_to_index)\n",
    "#         self.max_sequence_length = max_sequence_length\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "#         self.language_to_index = language_to_index\n",
    "#         self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "#         self.dropout = nn.Dropout(p=0.1)\n",
    "#         self.START_TOKEN = START_TOKEN\n",
    "#         self.END_TOKEN = END_TOKEN\n",
    "#         self.PADDING_TOKEN = PADDING_TOKEN\n",
    "    \n",
    "#     def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
    "\n",
    "#         def tokenize(sentence, start_token=True, end_token=True):\n",
    "#             sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "#             if start_token:\n",
    "#                 sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
    "#             if end_token:\n",
    "#                 sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
    "#             for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "#                 sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
    "#             return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "#         tokenized = []\n",
    "#         for sentence_num in range(len(batch)):\n",
    "#            tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
    "#         tokenized = torch.stack(tokenized)\n",
    "#         return tokenized.to(get_device())\n",
    "    \n",
    "#     def forward(self, x, end_token=True): # sentence\n",
    "#         x = self.batch_tokenize(x ,end_token)\n",
    "#         x = self.embedding(x)\n",
    "#         pos = self.position_encoder().to(get_device())\n",
    "#         x = self.dropout(x + pos)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
